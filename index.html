<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Potential Based Diffusion Motion Planning">
  <meta name="keywords" content="Machine Learning, ICML, Diffusion Model, Motion Planning, Compositionality, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Potential Based Diffusion Motion Planning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3593LNYVK9"></script>
<!-- TODO: -->
<script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-3593LNYVK9');
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="bootstrap-grid.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="load-mathjax.js" async></script>
</head>
<body>

<!-- Navigation Bar -->
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://devinluo27.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">
            Composable Diffusion
          </a>
          <a class="navbar-item" href="https://energy-based-model.github.io/comet/">
            COMET
          </a>
        </div>
      </div>
    </div>

  </div>


</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Potential Based Diffusion Motion Planning</h1>
            <!-- <h2 class="title is-size-2">ICCV 2023</h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://devinluo27.github.io/">Yunhao Luo</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chensun.me/">Chen Sun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yilundu.github.io/">Yilun Du</a><sup>2</sup>
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Brown University,</span>
            <span class="author-block"><sup>2</sup>MIT</span>
            <!-- <p>(* indicates equal contribution)</p> -->
            <br>ICML 2024
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- TODO: -->
                <a href="./potential_motion_plan.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- TODO: Arxiv-->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <!-- TODO: -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>

            Effective motion planning in high dimensional spaces is a long-standing open problem in robotics. One class of traditional motion planning algorithms corresponds to potential-based motion planning. An advantage of potential based motion planning is composability -- different motion constraints can be easily combined by adding corresponding potentials. However, constructing motion paths from potentials requires solving a global optimization across configuration space potential landscape,  which is often prone to local minima. We propose a new approach towards learning potential based motion planning, where we train a neural network to capture and learn an easily optimizable potentials over motion planning trajectories. We illustrate the effectiveness of such approach, significantly outperforming both classical and recent learned motion planning approaches and avoiding issues with local minima. We further illustrate its inherent composability, enabling us to generalize to a multitude of different motion constraints. 
            
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body centered">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="materials/unsupervised_discovery_teaser.m4v"
                type="video/mp4">
      </video> -->
      <figure>
          <img width="70%" src="materials/0-teaser-icml-website-v2.png">
        <p class="caption">
            <b>Illustrative Example of Composing Diffusion Energy Potentials.</b>
            Our approach learns different potential functions over motion planning trajectories $q_{1:N}$ (orange dashed lines). Different potentials can be combined and optimized to construct new motion plans that avoid obstacles encoded in both potential functions.

        </p>
    </figure>

    </div>
  </div>
</section>


 <!-- <p>
             We discover a set of compositional concepts given a dataset of unlabeled images. Score functions representing each concept $\{c^1, \dots, c^K\}$ are composed together to form a compositional score function that is trained to denoise images. The inferred concepts can be used to generate new images.
</p> -->

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
         
      <!-- <p> -->
      <!-- We next discuss how to learn potentials for potential motion planning that enable us to effectively optimize samples. -->
      <b>Learning Potentials for Motion Planning. </b>
      Given a motion plan $q_{1:T}$ from start state $q_{\text{st}}$ to end state $q_{\text{e}}$ and a characterization of the configuration space $C$ ($\textit{i.e.}$ the set of obstacles in the environment), we propose to learn a trajectory-level potential function $U_\theta$ so that
      <!-- </p>           -->
      \begin{equation}
      q_{1:T}^* = \text{arg\,min}_{q_{1:T}} U_\theta(q_{1:T}, q_{\text{st}}, q_{\text{e}}, C),
      \label{eqn:potential_traj}
      \end{equation} 

      where $q_{1:T}^*$ is a successful motion plan from $q_{\text{st}}$ to $q_{\text{e}}$. 
      To learn the potential function above, we propose to learn an 
      <a href="https://openai.com/index/energy-based-models/" class="citation-link">EBM</a>
      <!-- <a href="#ref1" class="citation-link">[1,</a> -->
      <!-- <a href="#ref2" class="citation-link">2]</a> -->
      <!-- ~\citep{lecun2006tutorial,du2019implicit}  -->
      across a dataset of solved motion planning $D = \{q_{\text{st}}^i, q_{\text{e}}^i,  q_{1:T}^i, C^i \}$, where $e^{-E_\theta(q_{1:T}|q_{\text{st}}, q_{\text{e}}, C)} \propto p(q_{1:T}|q_{\text{st}}, q_{\text{e}}, C)$. Since the dataset $D$ is of solved motion planning problems, the learned energy function $E_\theta$ will have minimal energy at successful motion plans $q_{1:T}^*$ and thus satisfy our potential function $U_\theta$. 
      <!-- in \Eqref{eqn:potential_traj}. -->
      <br/>
      <br/>
      <br/>

      <b>Learning EBM for Motion Plan Generation.</b>
      To learn the EBM landscape that enables us to effectively optimize and generate motion plans $q_{1:T}^*$, we propose to shape the energy landscape using <a href="https://arxiv.org/abs/1503.03585" class="citation-link">denoising diffusion</a> training objective.
      <!-- <a href="#ref3" class="citation-link">[1]</a> -->
      <!-- <a href="#ref3" class="citation-link">[3,</a> -->
      <!-- <a href="#ref4" class="citation-link">4]</a>. -->
      <!-- ~\citep{sohl2015deep,ho2020denoising}. -->

      Formally, to train our potential, we use the 
      <a href="https://energy-based-model.github.io/reduce-reuse-recycle/" class="citation-link">energy based diffusion</a>
       training objective,
       <!-- in <a href="#ref5" class="citation-link">[5]</a>, -->
      <!-- where the gradient of energy function is trained to denoise noise corrupted motion plans $q_{1:T}^*$, -->
      where the gradient of energy function is trained to denoise noise-corrupted motion plans $q_{1:T}^i$ from dataset $D$,

      \begin{equation}
      \mathcal{L}_{\text{MSE}}=\|\mathbf{\epsilon} - \nabla_{q_{1:T}} E_\theta(\sqrt{1-\beta_s} q_{1:T}^i +  \sqrt{\beta_s} \mathbf{\epsilon}, s, q_{\text{st}}^i, q_{\text{e}}^i, C^i)\|^2

      \label{eqn:train_obj}
      \end{equation}

      where $\epsilon$ is sampled from Gaussian noise $\mathcal{N}(0, 1)$,  $s \in\{1,2,...,S\}$ is the denoising diffusion step (we set $S = 100$), and $\beta_s$ is the corresponding Gaussian noise corruption on a motion planning path $q_{1:T}^i$. We refer to $E_\theta$ as the <i>diffusion potential function</i>.

      <br/>
      <br/>
      <br/>

      <b>Optimize and Sample from Diffusion Potential Function.</b>
      To optimize and sample from our diffusion potential function, we initialize a motion path $q_{1:T}^S$ at diffusion step $S$ from Gaussian noise $\mathcal{N}(0, 1)$ and optimize for motion path following the gradient of the energy function $E_{\theta}$. We iteratively refine the motion path $q_{1:T}^s$ across each diffusion step following 
      
      \begin{align}
      & q_{1:T}^{s-1}=q_{1:T}^{s}-\gamma \epsilon + \xi, \quad \xi \sim \mathcal{N} \bigl(0, \sigma^2_s I \bigl), \notag \\
      & \text{where} \: \: \epsilon =  \nabla_{q_{1:T}} E_\theta(q_{1:T}, s, q_{\text{st}}, q_{\text{e}}, C)
      \label{eqn:diffusion_opt}
      \end{align}

      To parameterize the energy function $E_\theta(q_{1:T}, s, q_{\text{st}}, q_{\text{e}}, C)$, 
      we use <a href="https://arxiv.org/abs/2207.12598" class="citation-link">classifier-free guidance</a>
      <!-- <a href="#ref6" class="citation-link">[6]</a> -->
      <!-- \cite{ho2022classifier} -->
       to form a peaker composite energy function conditioned on $C$.
      
      $\gamma$ and $\sigma^2_s$ are diffusion specific scaling constants (A rescaling term at each diffusion step is omitted above for clarity). The final predicted motion path $q_{1:T}^*$ corresponds to  the output $q_{1:T}^0$ after running $S$ steps of optimization from the diffusion potential function.

      
      <br/>
      <br/>
      <br/>
      <b>Composing Diffusion Potential Functions</b>

      Given two separate diffusion potential functions $E^1_\theta(\cdot)$ and $E^2_\theta(\cdot)$, encoding separate constraints for motion planning, we can likewise form a composite potential function $E^{\text{comb}}(\cdot) = E^1(\cdot) + E^2(\cdot)$ by directly summing the corresponding potentials. 
      To sample from the new diffusion potential function $E^{\text{comb}}$, we can follow 

        
      \begin{align}
          &q_{1:T}^{s-1}=q_{1:T}^{s}-\gamma \epsilon^{\text{comb}}+\xi, \quad \xi \sim \mathcal{N} \bigl(0, \sigma^2_s I \bigl), \label{eqn:diffusion_opt_comb} \\

          & \text{where} \: \: \epsilon^{\text{comb}} = \nabla_{q_{1:T}} (E_\theta^{1}(q_{1:T}, s, q_{\text{st}}, q_{\text{e}}, C_1) + E_\theta^{2}(q_{1:T}, s, q_{\text{st}}, q_{\text{e}}, C_2)). \notag
      \end{align}

      This potential function $E^{\text{comb}}$ will have low energy precisely at motion planning paths $q_{1:T}$ which satisfy both constraints, with sampling corresponding to optimizing this potential function. Hence, by iteratively sampling from the composite potential function, we can obtain motion plans that satisfy both constraints. Please see our 
      <a href="#comp-results-title" class="citation-link">compositional results</a> below.

      <!-- (see Appendix \ref{sect:app_cond_indep}) -->
      <!-- This composite potential is the ground truth potential combining constraints if the constraints are independent, which is satisfied if the set of trajectories given constraints are uniformly distributed, and otherwise serves as approximate proxy to optimize multiple constraints. -->





        </div>
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Trajectory Denoising Process using Diffusion Potential Function</h2>

        <!-- <div class="content has-text-justified has-text-centered"> -->
          <!-- <p> -->
            <!-- Our method can decompose a set of unlabeled images from into objects without using any labels. -->
          <!-- </p> -->
        <!-- </div> -->

        <!-- <img src="materials/decomposition/imagenet_decomposition.png" -->
             <!-- alt="Object Decomposition."/> -->
          <div class="video-container text-centered">
          
            <!-- <center> -->
            <video width="19%" playsinline="" autoplay="" preload="" muted="" loop>
                <source src="materials/rm2d-per-step/luotest_env6.mp4" type="video/mp4">
            </video>

            <video width="19%" playsinline="" autoplay="" preload="" muted="" loop>
              <source src="materials/rm2d-per-step/luotest_env10.mp4" type="video/mp4">
            </video>

            <video width="19%" playsinline="" autoplay="" preload="" muted="" loop>
              <source src="materials/rm2d-per-step/luotest_env17.mp4" type="video/mp4">
            </video>
          <!-- </center> -->
          <p>
            <!-- \textbf{Trajectory Denoising Process.}  -->
            <b>Trajectory Denoising Process on Maze2D.</b>
            Motion trajectories are randomly initialized from Gaussian distribution in timestep $S = 100$. Noises are iteratively removed via the gradient of the energy function. A feasible collision-free trajectory can be obtained at timestep $S = 0$.
            <!-- \eqref{eqn:diffusion_opt} -->
          </p>

          </div>
        
        
             <br/>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Motion Plans on Concave Obstacles</h2>

        <!-- <div class="content has-text-justified has-text-centered">
          <p>Our method can decompose the kitchen scenes into kitchen range (i.e., stove and microwave), kitchen island, and lighting effects.
                    <b>Note that we name each concept based on attention responses for easy visualization.</b> <u>Hover to image to visualize attention heat maps.</u>
                </p>
        </div> -->

        <!-- <h3 class="title is-6 has-text-centered">Silver</h3> -->
            <!-- <div class="container bg-3"> -->
                <!-- <div class="row"> -->
                    <!-- <div class="col-sm-2 text-center"> -->
                        <!-- <div class="figure"> -->
                            <!-- <img class="Sirv image-main img-fluid" src="materials/21-concave_compare-v4-icml.png" alt=""> -->
                        <!-- </div> -->

                  <!-- <div style="height: 500px"> -->
                    <div class="centered-container text-centered">
                    <!-- <center> -->
                      <!-- <div width="100%" class="figure centered-container" style="height: 600px"> -->
                      <!-- <div width="100%" class="centered-container"> -->
                        <!-- style="width:200%" -->
                          <!-- width="100%"  -->
                          <!-- <img src="materials/21-concave_compare-v4-icml.png" alt="" class="scaled-image-200"> -->
                          <!-- <img src="materials/21-concave_compare-v4-icml.png" alt="" style="width: 10000px" > -->
                          <!-- <img src="materials/21-concave_compare-v4-icml.png" alt="" style="transform: scale(2.1); transform-origin: top center;" > -->
                          <img src="materials/21-concave_compare-v4-icml-website.png" alt="" width="73%">

                          <!-- <div class="background-image" > </div> -->
                      <!-- </div> -->
                    <!-- </center> -->
                    <!-- <br/> -->
                  </div>
                  <br/>
                  <b> Qualitative Comparison of Motion Plans on Maze2D with Concave Obstacles.</b> 
                  The green star indicates the start pose and the red star indicates the goal pose.
                  RMP tends to stuck in local minima and fails to reach the goals; 
                  RRT* is slow in speed and may reach the planning time limit in some difficult problems; 
                  trajectories by MPNet may occasionally go through obstacles in the environments;
                  our method is able to generate smooth and low-cost trajectories without collision while being up to 10 times faster than BIT*.
                  <!-- <div class="caption"> -->
                  <!-- </div> -->

                    
                  <!-- </div> -->
                    <!-- </div> -->
                <!-- </div> -->
            <!-- </div> -->

        
        <!-- <h3 class="title is-6 has-text-centered">Silver</h3>
            <div class="container bg-3">
                <div class="row">
                    <div class="col-sm-2 text-center">
                        <div class="figure">
                            <img class="Sirv image-main img-fluid" src="materials/decomposition/ADE20K_decomposition/t5_6.png" alt="">
                            <img class="Sirv image-hover img-fluid" src="materials/decomposition/ADE20K_decomposition/t5_6_attn.png" alt="">
                        </div>
                    </div>
                </div>
            </div> -->
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Motion Plans on KUKA</h2>

        <div class="content has-text-justified has-text-centered">
          <!-- <p> -->
           
          <!-- </p> -->
        </div>

        <!-- <div class="container bg-3">
          <div class="row">
            <div class="video-container text-centered">

              <div class="col-lg-4 centered-caption">
                <figure>
                  <img src="materials/kuka7d/pbd-1-600.gif" alt="">
                  <!-- <figcaption class="text-centered"> asd </figcaption> -- >
                  <!-- <center-caption class="has-text-centered"> asd </center-caption> -- >
                  <!-- <center-caption class="centered"> asd </center-caption> -- >
                  <!-- <center-caption> asd </center-caption> -- >
                  <!-- <div class="centered-caption">Ours</div> -- >
                  Ours
                  <!-- <div class="text-centered">asd</div> -- >
                </figure>

                <!-- <div class="caption">Ours</div> -- >
                <!-- <caption>asd</caption> -- >
              </div>

              <div class="col-lg-4 centered-caption">
                <img src="materials/kuka7d/mpo-1-600.gif" alt="">
                  MπNet
              </div>
            
            </div>
          </div>
        </div> -->




        <!-- <br/> -->

        <!-- Two rows layout -->
        <!-- <div class="container bg-3">
          <div class="row">

            <div class="video-container">
              <div class="col-md-4 centered-caption">
                <figure>
                  <img src="materials/kuka7d/pbd-2-600.gif" alt="">
                  Ours
                </figure>
              </div>

              <div class="col-lg-4 centered-caption">
                <img src="materials/kuka7d/mpo-2-600.gif" alt="">
                  MπNet
              </div>
            </div>

          </div>
        </div> -->

        <!-- NEW June 6, one row -->
        <div class="container bg-3">
          <div class="row">
            <div class="col-sm-6 centered-caption"  style="font-size: 16pt;">
              (a)
            </div>
            <div class="col-sm-6 centered-caption" style="font-size: 16pt;">
              (b)
            </div>
          </div>
        </div>
          
        <div class="container bg-3">
          <div class="row">

            <!-- <div class="video-container"> -->
            <!-- Problem 1 -->
              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/pbd-1-600.mp4" type="video/mp4">
                </video>
                Ours
              </div>


              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/mpo-1-600.mp4" type="video/mp4">
                </video>
                MπNet
              </div>

              <!-- Problem 2 -->
              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/pbd-2-600.mp4" type="video/mp4">
                </video>
                Ours
              </div>


              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/mpo-2-600.mp4" type="video/mp4">
                </video>
                MπNet
              </div>
            <!-- </div> -->

          </div>
        </div>
        <br/>


        <div class="container bg-3">
          <div class="row">
            <div class="col-sm-6 centered-caption"  style="font-size: 16pt;">
              (c)
            </div>
            <div class="col-sm-6 centered-caption" style="font-size: 16pt;">
              (d)
            </div>
          </div>
        </div>
          
        <div class="container bg-3">
          <div class="row">

            <!-- <div class="video-container"> -->
            <!-- Problem 1 -->
              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/pbd-3-600.mp4" type="video/mp4">
                </video>
                Ours
              </div>


              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/mpo-3-600.mp4" type="video/mp4">
                </video>
                MπNet
              </div>

              <!-- Problem 2 -->
              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/pbd-4-600.mp4" type="video/mp4">
                </video>
                Ours
              </div>


              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/mpo-4-600.mp4" type="video/mp4">
                </video>
                MπNet
              </div>
            <!-- </div> -->

          </div>
        </div>
        
        <b> Qualitative Comparison of Motion Plans on KUKA.</b> 
        We present motion plans by our diffusion potential planner and <a href="https://mpinets.github.io/" class="citation-link">M$\pi$Net</a>.
        The large green/pink ball indicates the start/goal state. Our method in the left column generates smooth and near-optimal trajectories, while the trajectories of M$\pi$Net might choose a longer route, get stuck in some local regions, fail to pass through some narrow passages, or keep hovering near the start state.



        
      </div>
    </div>
  </div>
</section>








<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-4"> Flexible Trajectory Morphology</h2> -->
        <h2 class="title is-4"> Diverse Motion Plan Morphology</h2>


        <div class="container bg-3">
          <div class="row centered-caption">
            
            <!-- TODO: Add More, these two videos start sec = 1, while others = 2. -->
            <div class="video-container" style="gap: 0px">
              
              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/pbd-6-600-morp1.mp4" type="video/mp4">
                </video>
                Trajectory 1
              </div>


              <div class="col-sm-3 centered-caption">
                <video playsinline="" autoplay="" preload="" muted="" loop controls>
                  <source src="materials/kuka7d/pbd-6-600-morp2.mp4" type="video/mp4">
                </video>
                Trajectory 2
              </div>

            </div>
            <div class="left-caption">
            <!-- <b>Diverse Motion Plan Morphology.</b> -->
            <b>Qualitative Visualizations of Different Trajectory Morphology.</b>
            Our method can generate trajectories with various morphological shapes. The large green/pink ball indicates the location of the end effector of the start/goal state. Trajectory 1 and Trajectory 2 are generated under the same constraints (e.g., start state, goal state, obstacles locations) but with different starting noise, resulting in different route selection. Trajectory 1 routes through the central narrow passage between the two purple blocks and arrives the pink ball from above. Trajectory 2 avoids the obstacles by going under the blocks and finally arrives the same goal state from below.
            </div>
            
          </div>
        </div>

        

        
      </div>
    </div>
  </div>
</section>




<hr>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-4">Compositional Generalization over Increasing Obstacles</h2> -->
        <h2 class="title is-4" id="comp-results-title">Generalization via Composing Potentials</h2>

        <div class="content has-text-justified has-text-centered">

          <p>
           Our method can generalize to more difficult out-of-distribution environments by sampling from the composite diffusion potential function. We demonstrate qualitative results where we compose potentials of the same obstacle type, of different obstacle types, and of static and dynamic obstacles. Please see our paper for more comprehensive results.
          </p>


        </div>
        <!-- <img src="materials/40-comp_rm2d-3.png" width="75%" alt="Composition."/> -->

        <div class="centered-caption"  width="75%">
          <figure>
            <img width="75%" src="materials/40-comp_rm2d-3-web.png" alt="">
            <br/>
            <div class="left-caption">
              <b>Compositional Generalization over Increasing Obstacles.</b>
              The green/pink star indicates the start/goal state. Our planner is
              trained with a dataset with only 6 obstacles and can generalize to scenarios with more obstacles by directly composing potentials in test
              time (without any re-training). 
              Though trained on environments with fewer obstacles, the generated trajectories can still reach the goals with near optimal paths and various morphology shapes through composing potentials.
            </div>
          </figure>
        </div>
        <br/>
        <br/>

        <div class="centered-caption">
          <figure>
            <img width="55%" src="materials/43-comp_rm2d-diff-v2-web.png" alt="">
            <br/>

            <div class="left-caption">
              <b>Compositional Generalization over Different Obstacles.</b>
              Two separately trained models are composed: a model <i>only</i> trained on large blocks (blue) of size $1.4 \times 1.4$ and a model <i>only</i> trained on smaller blocks (orange) $1 \times 1$. By composing two models, our planner can generalize to more complex scenarios with various obstacles in test time (without any re-training). The generated trajectories demonstrate various morphology and reach the goal with smooth and short paths. 
            </div>
          </figure>
        </div>
        <br/>
        <br/>


        <div class="centered-caption">
          <figure>
            <img width="55%" src="materials/45-comp_rm2d-diff-concave-web.png" alt="">
            <br/>
            <div class="left-caption">
              <b>Compositional Generalization over Square and Concave Obstacles.</b> 
              Two separately trained models are composed: a model <i>only</i> trained on square blocks (blue) of size $1.0 \times 1.0$ and a model <i>only</i> trained on concave blocks (orange). 
              The composite planner is well aware of the geometrical difference between the square and concave obstacles and is able to propose successful and near-optimal motion plans accordingly. 
            </div>
          </figure>
        </div>
        <br/>
        <br/>


        <div class="centered-caption">
          <div class="container bg-3">
            <div class="row">
              <div class="video-container" style="gap: 0px">

                <div class="col-20 centered-caption">
                  <video playsinline="" autoplay="" preload="" muted="" loop controls>
                    <source src="materials/comp_dyn/comp_dyn_s6d1_1.mp4" type="video/mp4">
                  </video>
                  <!-- Ours -->
                </div>
  
  
                <div class="col-20 centered-caption">
                  <video playsinline="" autoplay="" preload="" muted="" loop controls>
                    <source src="materials/comp_dyn/comp_dyn_s6d1_2.mp4" type="video/mp4">
                  </video>
                  <!-- MπNet -->
                </div>
  
                <!-- Problem 2 -->
                <div class="col-20 centered-caption">
                  <video playsinline="" autoplay="" preload="" muted="" loop controls>
                    <source src="materials/comp_dyn/comp_dyn_s3d1_1.mp4" type="video/mp4">
                  </video>
                  <!-- Ours -->
                </div>
  
  
                <div class="col-20 centered-caption">
                  <video playsinline="" autoplay="" preload="" muted="" loop controls>
                    <source src="materials/comp_dyn/comp_dyn_s3d1_2.mp4" type="video/mp4">
                  </video>
                  <!-- MπNet -->
                </div>
              <!-- </div> -->
              </div>
  
            </div>
          </div>

          <div class="container bg-3">
            <div class="row" style="margin-top: -15px;">

              <div style="display: flex; justify-content: center; align-items: center; white-space: nowrap; width: 100%;">
                
                <div class="col-40 centered-caption" style="font-size: 13pt;">
                  6 static + 1 dynamic
                </div>

                <div class="col-40 centered-caption" style="font-size: 13pt;">
                  3 static + 1 dynamic
                </div>

              </div>

            </div>
          </div>


          <div class="left-caption">
            <b>Compositional Generalization over Static and Dynamics Obstacles. </b> 
            The current position of the agent is shown in the pink asterisk. The gray line indicates the moving trajectory of the dynamic obstacle (the large blue block). 
            Note that our method is only trained on environments that purely consist of static obstacles or dynamic obstacles. For example, for the left two videos, we compose a model trained on 6 static obstacles and a model trained on 1 dynamic obstacle and evaluate the model on environments with both static and dynamic obstacles. 
            The composite planner is able to generate feasible motion plans, where
            <!-- avoid both the static obstacles and dynamic obstacle.  -->
            the resulting trajectories do not directly go straight to the targets but detour and wait to avoid the moving obstacle.
            
          </div>

          <!-- <figure>
            <img width="55%" src="materials/45-comp_rm2d-diff-concave-web.png" alt="">
            <br/>
            <div class="left-caption">
              <b>Compositional Generalization over Static and Dynamics Obstacles.</b> 
              
            </div>
          </figure> -->

        </div>

        
        <br/>
      </div>
    </div>
  </div>
</section>

<hr>

<!-- TODO: add motion plans of real world datasets -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content column is-full-width ">
    <!-- TODO: -->
    <h2 class="title is-4">BibTeX</h2>
    <pre><code> Coming Soon.
      <!-- @InProceedings{Liu_2023_ICCV,
    author    = {Liu, Nan and Du, Yilun and Li, Shuang and Tenenbaum, Joshua B. and Torralba, Antonio},
    title     = {Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {2085-2095}
} -->
</code></pre>
  </div>
</section>



<section class="section">
        <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Related Projects</h2>
          Check out a list of our related papers on compositional generation and energy based models. A full list can be found <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">here</a>!
          <br>
          <br>
        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/recyle.m4v" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/reduce-reuse-recycle/">Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</a>
        </div>
        <div>
            We propose new samplers, inspired by MCMC, to enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers.
        </div>
        </div>
        </div>
          <br>




        <div class="row vspace-top">
        <div class="col-sm-3">
            <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/teaser_glide.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Compositional Visual Generation with Composable Diffusion Models</a>
        </div>
        <div>
            We present a method to compose different diffusion models together, drawing on the close connection of
            diffusion models with EBMs. We illustrate how compositional operators enable
            the ability to composing multiple sets of objects together as well as generate images subject to
            complex text prompts.
        </div>
        </div>
        </div>
        <br>


        

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/related/diffuser_schematic-01.png" class="img-fluid" alt="comp_carton" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://diffusion-planning.github.io/">Planning with Diffusion for Flexible Behavior Synthesis</a>
        </div>
        <div>
          Diffuser is a denoising diffusion probabilistic model that plans by iteratively refining randomly sampled noise. The denoising process lends itself to flexible conditioning, by either using gradients of an objective function to bias plans toward high-reward regions or conditioning the plan to reach a specified goal.
        </div>
        </div>
        </div>
      </div>
    </div>
        </div>
</section>


<!-- Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015. -->

<!-- 
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">References</h2>

        <ol>
          <li id="ref1" class="reference">
            LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.
          </li>

          <li id="ref2" class="reference">
            Du, Y. and Mordatch, I. Implicit generation and modeling with energy based models. Advances in Neural Information Processing Systems, 32, 2019.
          </li>

          <li id="ref3" class="reference">
            Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015.
          </li>

          <li id="ref4" class="reference">
            Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pp. 6840–6851, 2020.
          </li>

          <li id="ref5" class="reference">
            Du, Y., Durkan, C., Strudel, R., Tenenbaum, J. B., Dieleman, S., Fergus, R., Sohl-Dickstein, J., Doucet, A., and Grathwohl, W. S. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International Conference on Machine Learning, pp.8489–8510. PMLR, 2023.
          </li>

          <li id="ref6" class="reference">
            Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.
          </li>

      </ol>

        
      </div>
    </div>
  </div>
</section> -->




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- TODO: -->
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
        <div class="content">
          <p>
            This website is forked from the
						<a href="https://nerfies.github.io/">Nerfies</a> and
						<a href="https://github.com/nerfies/nerfies.github.io">source code</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
